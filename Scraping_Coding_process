Coding Process

We began by identifying the relevant HTML elements on the target website that contained the data we aimed to scrape from the tables — 
specifically the locinfodiv and newlocminlist sections.

Initially, we wrote a basic script  that was capable of scraping data from a single locality. 
This was only successful after switching from BeautifulSoup4 to Selenium, paired with the ChromeDriver, 
since the content was not rendered in the static HTML and therefore not accessible through BeautifulSoup.

Subsequently, we extended this code to allow it to first collect all relevant links to individual localities
displayed on the map and then to extract the information from each of these links, storing the results in a structured table format.

After a brief test run—where we initially limited the script to collecting only three links—we launched the extended code. 
However, several issues arose. One of the main problems was that the browser kept closing unexpectedly without any error messages or explanation.
In response, we implemented multiple try and except blocks to catch and handle potential errors during execution. 
While this mitigated some issues, it did not fully resolve the problem. 

Because the website is designed to prevent automated scraping, we made further adjustments to disguise the script’s behavior: we introduced rotating user agents, 
installed undetected_chromedriver, and incorporated randomized wait times (between 5 and 15 seconds) between requests to simulate more human-like behavior.

Despite these measures, the script still terminated unexpectedly after approximately one hour of execution. 
After installing a VPN provider, the issue was finally resolved, and the script was able to run for five consecutive hours, 
successfully collecting data from around 520 links.

Unfortunately, the scraping logic—designed to navigate through the raw HTML structure rather than zooming and scrolling 
the map manually—sometimes only captured summary pages instead of individual locality pages. 
This occurred in cases where Mindat displayed an overview of localities within a region, rather than listing them individually.

To investigate this, we re-scraped one of the summary pages and observed that, initially, only the aggregate page was shown. 
However, after increasing the wait times before scraping individual localities, the full list of detailed localities became accessible.

Due to time constraints, we did not rerun the script to scrape these pages individually, nor did we extend the code to reprocess all previously collected pages.

Finally, it is worth noting that Mindat does offer a public API, but it did not provide the specific data we required. 
Therefore, we decided to proceed with the dataset gathered through our customized scraping process.
